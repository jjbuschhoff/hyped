import datasets
import pyarrow as pa
from .processors.base import BaseDataProcessor
from hyped.utils.arrow import convert_features_to_arrow_schema
from typing import Any


class DataPipe(list):
    """Data Pipe

    A Data Pipe is a sequence of data processors. It provides useful
    functionality such as passing a batch of examples through the
    sequence or pipe.

    Arguments:
        processors (list[BaseDataProcessor]): the initial pipe of processors
    """

    def __init__(self, processors: list[BaseDataProcessor] = []) -> None:
        # check types of processors
        if not all(isinstance(p, BaseDataProcessor) for p in processors):
            raise TypeError(
                "All processors used in a data pipe must inherit from `%s`"
                % BaseDataProcessor
            )
        # initialize pipe as list of processors
        list.__init__(self, processors)

    def prepare(self, features: datasets.Features) -> datasets.Features:
        """Prepare all data processors of the data pipe for execution

        Arguments:
            features (Features):
                input dataset features available to the processor on execution

        Returns:
            out_features (Features):
                dataset features of the output of the processor
        """
        # prepare all processors
        for p in self:
            features = p.prepare(features)
        # return final output features
        return self.out_features

    @property
    def is_prepared(self) -> bool:
        """Check if the data pipe is prepared and ready for execution
        This also verifies the feature pipe, i.e. checks that the output
        of any processor matches the input of the following one.
        """
        return (
            # check all processors of the pipe
            all(p.is_prepared for p in self)
            and all(
                p1.out_features == p2.in_features
                for p1, p2 in zip(self[:-1], self[1:])
            )
        )

    @property
    def in_features(self) -> datasets.Features:
        """Input dataset features available to data pipe"""
        return self[0].in_features

    @property
    def new_features(self) -> datasets.Features:
        """New dataset features generated by data pipe"""
        # aggregate all new features created through the pipe
        features = datasets.Features()
        for p in self:
            features.update(p.new_features)

        return features

    @property
    def out_features(self) -> datasets.Features:
        """All output features of the processor. Includes both input
        features and new features generated by the data pipe. On conflicts,
        the new features are prioritized.
        """
        return self[-1].out_features

    def batch_process(
        self, examples: dict[str, list[Any]], index: list[int], rank: int
    ) -> dict[str, list[Any]]:
        """Process a batch of examples

        Arguments:
            examples (dict[str, list[Any]]): batch of examples to process
            index (list[int]): dataset indices of the examples
            rank (int): execution process rank

        Returns:
            out (dict[str, list[Any]]): processed examples
        """
        # make sure the pipeline is prepared
        if not self.is_prepared:
            raise RuntimeError(
                "Data Pipe is not prepared. This happens either when the "
                "`prepare` function was not called, or when a processor "
                "of the pipe is re-prepared with different features."
            )
        # apply each processor
        for p in self:
            examples, index = p.batch_process(
                examples, index, rank, return_index=True
            )
        # return final output
        return examples

    def _batch_process_to_pyarrow(
        self, examples: dict[str, list[Any]], index: list[int], rank: int
    ) -> pa.Table:
        # convert to pyarrow table with correct schema
        return pa.table(
            data=self.batch_process(examples, index, rank),
            schema=convert_features_to_arrow_schema(self.out_features),
        )

    def apply(
        self, data: datasets.Dataset | datasets.DatasetDict, **kwargs
    ) -> datasets.Dataset | datasets.DatasetDict:
        """Apply the data pipe to a dataset

        Arguments:
            data (datasets.Dataset|datasets.DatasetDict): source dataset(s)
            **kwargs (dict[str, Any]):
                arguments forwarded to datasets `.map` function

        Returns:
            out (datasets.Dataset|datasets.DatasetDict): processed dataset(s)
        """
        # prepare for the dataset
        if isinstance(data, datasets.Dataset):
            self.prepare(data.features)
        elif isinstance(data, datasets.DatasetDict):
            self.prepare(next(iter(data.values())).features)
        else:
            raise ValueError(
                "Expected a `datasets.Dataset` or `datasets.DatasetDict`, "
                "got %s" % type(data)
            )

        # required settings
        kwargs["batched"] = True
        kwargs["with_indices"] = True
        kwargs["with_rank"] = True
        # apply data pipe
        return data.map(self._batch_process_to_pyarrow, **kwargs)
